{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaz38D8iQUpA+HknleN5RO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haripriya-Mahajan/AI-Model-Comparison-Tool/blob/main/AIModelComparisonTool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install PyPDF2 pandas requests\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "\n",
        "# ===============================\n",
        "# CONFIG\n",
        "# ===============================\n",
        "CACHE_DIR = \"/content/drive/MyDrive/LLM_Project_Cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "PDF_CACHE_PATH = os.path.join(CACHE_DIR, \"pdf_texts.pkl\")\n",
        "COMPRESSED_CACHE_PATH = os.path.join(CACHE_DIR, \"pdf_texts_compressed.pkl\")\n",
        "CSV_CACHE_PATH = os.path.join(CACHE_DIR, \"benchmark_data.pkl\")\n",
        "API_CACHE_PATH = os.path.join(CACHE_DIR, \"aa_api_models.pkl\")\n",
        "\n",
        "# ===============================\n",
        "# PDF EXTRACTION\n",
        "# ===============================\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f, strict=False)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except:\n",
        "        print(f\"‚ö†Ô∏è Skipping PDF: {pdf_path}\")\n",
        "    return text\n",
        "\n",
        "# ===============================\n",
        "# SCALEDOWN COMPRESSION\n",
        "# ===============================\n",
        "def compress_texts_with_scaledown(texts, api_key):\n",
        "    compressed = []\n",
        "    headers = {\"x-api-key\": api_key, \"Content-Type\": \"application/json\"}\n",
        "    for t in texts:\n",
        "        payload = {\n",
        "            \"context\": t[:12000],  # safety cap\n",
        "            \"prompt\": \"Compress for AI research use\",\n",
        "            \"scaledown\": {\"rate\": \"auto\"}\n",
        "        }\n",
        "        try:\n",
        "            r = requests.post(\"https://api.scaledown.xyz/compress/raw/\",\n",
        "                              headers=headers, json=payload, timeout=60)\n",
        "            if r.status_code == 200 and \"compressed_prompt\" in r.json():\n",
        "                compressed.append(r.json()[\"compressed_prompt\"])\n",
        "            else:\n",
        "                compressed.append(t)\n",
        "        except:\n",
        "            compressed.append(t)\n",
        "    return compressed\n",
        "\n",
        "# ===============================\n",
        "# LOAD OR UPLOAD PDFs\n",
        "# ===============================\n",
        "if os.path.exists(PDF_CACHE_PATH):\n",
        "    with open(PDF_CACHE_PATH, \"rb\") as f:\n",
        "        pdf_texts = pickle.load(f)\n",
        "    print(\"‚úÖ Loaded cached PDFs\")\n",
        "else:\n",
        "    from google.colab import files\n",
        "    print(\"üì§ Upload PDF files\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_texts = []\n",
        "    for file_name in uploaded:\n",
        "        if file_name.lower().endswith(\".pdf\"):\n",
        "            pdf_texts.append(extract_text_from_pdf(file_name))\n",
        "    with open(PDF_CACHE_PATH, \"wb\") as f:\n",
        "        pickle.dump(pdf_texts, f)\n",
        "    print(\"‚úÖ PDFs cached\")\n",
        "\n",
        "\n",
        "SCALEDOWN_API_KEY = input(\"Enter Scaledown API key (or press Enter to skip compression): \").strip()\n",
        "if SCALEDOWN_API_KEY:\n",
        "    if os.path.exists(COMPRESSED_CACHE_PATH):\n",
        "        with open(COMPRESSED_CACHE_PATH, \"rb\") as f:\n",
        "            pdf_texts = pickle.load(f)\n",
        "        print(\"‚úÖ Loaded cached compressed PDF texts\")\n",
        "    else:\n",
        "        print(\"üåê Compressing PDF texts with Scaledown API...\")\n",
        "        pdf_texts = compress_texts_with_scaledown(pdf_texts, SCALEDOWN_API_KEY)\n",
        "        with open(COMPRESSED_CACHE_PATH, \"wb\") as f:\n",
        "            pickle.dump(pdf_texts, f)\n",
        "        print(\"‚úÖ Compressed texts cached\")\n",
        "\n",
        "# ===============================\n",
        "# LOAD OR UPLOAD CSV BENCHMARK\n",
        "# ===============================\n",
        "if os.path.exists(CSV_CACHE_PATH):\n",
        "    with open(CSV_CACHE_PATH, \"rb\") as f:\n",
        "        benchmark_df = pickle.load(f)\n",
        "    print(\"‚úÖ Loaded cached benchmark CSV\")\n",
        "else:\n",
        "    from google.colab import files\n",
        "    print(\"üì§ Upload benchmark CSV\")\n",
        "    uploaded = files.upload()\n",
        "    csv_file = [f for f in uploaded if f.lower().endswith(\".csv\")][0]\n",
        "    benchmark_df = pd.read_csv(csv_file)\n",
        "    with open(CSV_CACHE_PATH, \"wb\") as f:\n",
        "        pickle.dump(benchmark_df, f)\n",
        "    print(\"‚úÖ CSV cached\")\n",
        "\n",
        "# ===============================\n",
        "# ARTIFICIAL ANALYSIS API\n",
        "# ===============================\n",
        "API_KEY = input(\"Enter Artificial Analysis API key (or press Enter to skip): \").strip()\n",
        "api_df = pd.DataFrame()\n",
        "if API_KEY:\n",
        "    if os.path.exists(API_CACHE_PATH):\n",
        "        with open(API_CACHE_PATH, \"rb\") as f:\n",
        "            api_df = pickle.load(f)\n",
        "        print(\"‚úÖ Loaded cached API data\")\n",
        "    else:\n",
        "        print(\"üåê Fetching data from Artificial Analysis API...\")\n",
        "        url = \"https://artificialanalysis.ai/api/v2/data/llms/models\"\n",
        "        headers = {\"x-api-key\": API_KEY}\n",
        "        r = requests.get(url, headers=headers)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()[\"data\"]\n",
        "            rows = []\n",
        "            for m in data:\n",
        "                rows.append({\n",
        "                    \"model\": m[\"name\"],\n",
        "                    \"creator\": m[\"model_creator\"][\"name\"],\n",
        "                    \"intelligence\": m[\"evaluations\"].get(\"artificial_analysis_intelligence_index\"),\n",
        "                    \"coding\": m[\"evaluations\"].get(\"artificial_analysis_coding_index\"),\n",
        "                    \"math\": m[\"evaluations\"].get(\"artificial_analysis_math_index\"),\n",
        "                    \"speed\": m.get(\"median_output_tokens_per_second\"),\n",
        "                    \"latency\": m.get(\"median_time_to_first_token_seconds\")\n",
        "                })\n",
        "            api_df = pd.DataFrame(rows)\n",
        "            with open(API_CACHE_PATH, \"wb\") as f:\n",
        "                pickle.dump(api_df, f)\n",
        "            print(\"‚úÖ API data cached\")\n",
        "        else:\n",
        "            print(\"‚ùå Failed to fetch API data\")\n",
        "\n",
        "# ===============================\n",
        "# QUERY HANDLING\n",
        "# ===============================\n",
        "CANONICAL_MODELS = {\n",
        "    \"gpt\": \"GPT-4\",\n",
        "    \"openai\": \"GPT-4\",\n",
        "    \"claude\": \"Claude 3 Opus\",\n",
        "    \"gemini\": \"Gemini 1.5 Pro\",\n",
        "    \"google\": \"Gemini 1.5 Pro\",\n",
        "    \"llama\": \"LLaMA 3\"\n",
        "}\n",
        "\n",
        "def detect_intent(query):\n",
        "    q = query.lower()\n",
        "    if any(k in q for k in [\"accuracy\", \"intelligence\", \"performance\", \"reasoning\"]):\n",
        "        return \"intelligence\"\n",
        "    if any(k in q for k in [\"speed\", \"fast\"]):\n",
        "        return \"speed\"\n",
        "    if \"latency\" in q:\n",
        "        return \"latency\"\n",
        "    return \"research\"\n",
        "\n",
        "# ===============================\n",
        "# Extract models by family and get API variants\n",
        "# ===============================\n",
        "def extract_models(query):\n",
        "    found = []\n",
        "    query_lower = query.lower()\n",
        "    for keyword, canonical_name in CANONICAL_MODELS.items():\n",
        "        if keyword in query_lower:\n",
        "            # find all API variants that belong to this canonical family\n",
        "            variants = api_df[api_df['model'].str.contains(canonical_name, case=False)]\n",
        "            for v in variants['model']:\n",
        "                if v not in found:\n",
        "                    found.append(v)\n",
        "    return found\n",
        "\n",
        "\n",
        "def bar(score, scale=2):\n",
        "    return \"‚ñà\" * (score // scale)\n",
        "\n",
        "def answer_query(query):\n",
        "    intent = detect_intent(query)\n",
        "    mentioned_models = extract_models(query)\n",
        "\n",
        "    # ---------- COMPARISON MODE ----------\n",
        "    if mentioned_models and not api_df.empty:\n",
        "        df = api_df[api_df['model'].isin(mentioned_models)]\n",
        "        if df.empty:\n",
        "            return \"‚ùå None of the mentioned canonical models were found in the database.\"\n",
        "        display_col = {\"intelligence\":\"intelligence\",\"speed\":\"speed\",\"latency\":\"latency\"}.get(intent, \"intelligence\")\n",
        "        answer = f\"\\nüìä COMPARISON OF SELECTED MODELS BY {intent.upper()}\\n\\n\"\n",
        "        for i, row in df.iterrows():\n",
        "            val = row[display_col]\n",
        "            bar_val = bar(int(val)) if display_col==\"intelligence\" and not pd.isna(val) else \"\"\n",
        "            if pd.isna(val):\n",
        "                val = \"N/A\"\n",
        "            answer += f\"{row['model']:<25} | {val:>7} {bar_val}\\n\"\n",
        "        return answer\n",
        "\n",
        "    # ---------- TOP MODELS MODE ----------\n",
        "    if intent in [\"intelligence\",\"speed\",\"latency\"] and not api_df.empty:\n",
        "        col = {\"intelligence\":\"intelligence\",\"speed\":\"speed\",\"latency\":\"latency\"}[intent]\n",
        "        df = api_df.dropna(subset=[col])\n",
        "        ascending = True if intent==\"latency\" else False\n",
        "        df = df.sort_values(col, ascending=ascending).head(10)\n",
        "        if df.empty:\n",
        "            return f\"‚ùå No {intent} data available.\"\n",
        "        title = {\n",
        "            \"intelligence\":\"TOP MODELS BY INTELLIGENCE\",\n",
        "            \"speed\":\"FASTEST MODELS (Tokens/sec)\",\n",
        "            \"latency\":\"LOWEST LATENCY MODELS (sec)\"\n",
        "        }[intent]\n",
        "        answer = f\"\\nüìä {title} (Artificial Analysis)\\n\\n\"\n",
        "        for i, row in df.iterrows():\n",
        "            val = row[col]\n",
        "            bar_val = bar(int(val)) if intent==\"intelligence\" else \"\"\n",
        "            answer += f\"{row['model']:<25} | {val:>7} {bar_val}\\n\"\n",
        "        return answer\n",
        "\n",
        "    # ---------- FALLBACK: RESEARCH ----------\n",
        "    keywords = re.findall(r\"\\w+\", query.lower())\n",
        "    pdf_hits = []\n",
        "    for text in pdf_texts:\n",
        "        score = sum(text.lower().count(k) for k in keywords)\n",
        "        if score > 0:\n",
        "            pdf_hits.append((score, text[:500]))\n",
        "    pdf_hits.sort(reverse=True)\n",
        "\n",
        "    csv_hits = benchmark_df[benchmark_df.apply(\n",
        "        lambda row: any(k in str(row).lower() for k in keywords),\n",
        "        axis=1\n",
        "    )]\n",
        "\n",
        "    answer = \"\\nüìå RESULTS FROM RESEARCH DATA\\n\\n\"\n",
        "    if not csv_hits.empty:\n",
        "        answer += \"üîπ Benchmarks:\\n\"\n",
        "        answer += csv_hits.head(5).to_string(index=False)\n",
        "        answer += \"\\n\\n\"\n",
        "    if pdf_hits:\n",
        "        answer += \"üîπ Paper Evidence:\\n\"\n",
        "        answer += pdf_hits[0][1] + \"\\n\"\n",
        "    if csv_hits.empty and not pdf_hits:\n",
        "        answer += \"No relevant information found.\"\n",
        "    return answer\n",
        "\n",
        "# ===============================\n",
        "# RUN\n",
        "# ===============================\n",
        "user_query = input(\"Enter your query: \")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(answer_query(user_query))\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ShJ7aRrtXvK",
        "outputId": "46276e5d-3215-44da-8c77-5f31236ca082"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "‚úÖ Loaded cached PDFs\n",
            "Enter Scaledown API key (or press Enter to skip compression): \n",
            "‚úÖ Loaded cached benchmark CSV\n",
            "Enter Artificial Analysis API key (or press Enter to skip): aa_oLvRzBgzLlSqPWjaNvNSvgwZMoHQAZcJ\n",
            "‚úÖ Loaded cached API data\n",
            "Enter your query: Which model has lower latency, gpt or gemini?\n",
            "\n",
            "============================================================\n",
            "\n",
            "üìä COMPARISON OF SELECTED MODELS BY LATENCY\n",
            "\n",
            "GPT-4o (Aug '24)          |   0.548 \n",
            "GPT-4o (May '24)          |    0.53 \n",
            "GPT-4 Turbo               |    1.24 \n",
            "GPT-4o (Nov '24)          |   0.436 \n",
            "GPT-4o mini               |   0.555 \n",
            "GPT-4                     |   0.951 \n",
            "GPT-4.1 mini              |   0.462 \n",
            "GPT-4.1                   |   0.473 \n",
            "GPT-4.1 nano              |   0.372 \n",
            "GPT-4o mini Realtime (Dec '24) |     0.0 \n",
            "GPT-4o Realtime (Dec '24) |     0.0 \n",
            "GPT-4.5 (Preview)         |     0.0 \n",
            "GPT-4o (ChatGPT)          |   0.458 \n",
            "GPT-4o (March 2025, chatgpt-4o-latest) |   0.466 \n",
            "Gemini 1.5 Pro (Sep '24)  |     0.0 \n",
            "Gemini 1.5 Pro (May '24)  |     0.0 \n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}