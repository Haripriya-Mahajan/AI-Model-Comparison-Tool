{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMF97Q3kijxSplZOhsEEbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haripriya-Mahajan/AI-Model-Comparison-Tool/blob/main/AIModelComparisonTool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# LLM Research Project (NO OpenAI, FIXED)\n",
        "# =========================\n",
        "\n",
        "# 1ï¸âƒ£ Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install PyPDF2 pandas requests\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# -------------------------\n",
        "# 2ï¸âƒ£ Paths\n",
        "# -------------------------\n",
        "CACHE_DIR = \"/content/drive/MyDrive/LLM_Project_Cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "PDF_CACHE_PATH = os.path.join(CACHE_DIR, \"pdf_texts.pkl\")\n",
        "CSV_CACHE_PATH = os.path.join(CACHE_DIR, \"benchmark_data.pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# 3ï¸âƒ£ PDF text extraction\n",
        "# -------------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f, strict=False)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Skipping corrupted PDF: {pdf_path}\")\n",
        "        print(f\"   Reason: {e}\")\n",
        "\n",
        "    if not text.strip():\n",
        "        print(f\"âš ï¸ No extractable text found in {pdf_path}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# -------------------------\n",
        "# 4ï¸âƒ£ Load or upload PDFs\n",
        "# -------------------------\n",
        "if os.path.exists(PDF_CACHE_PATH):\n",
        "    with open(PDF_CACHE_PATH, \"rb\") as f:\n",
        "        pdf_texts = pickle.load(f)\n",
        "    print(\"âœ… Loaded cached PDF texts from Drive\")\n",
        "else:\n",
        "    from google.colab import files\n",
        "    print(\"ðŸ“¤ Upload ONLY PDF files now\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    pdf_texts = []\n",
        "\n",
        "    for file_name in uploaded:\n",
        "        if file_name.lower().endswith(\".pdf\"):\n",
        "            text = extract_text_from_pdf(file_name)\n",
        "            pdf_texts.append(text)\n",
        "            print(f\"Extracted {len(text)} characters from {file_name}\")\n",
        "\n",
        "    with open(PDF_CACHE_PATH, \"wb\") as f:\n",
        "        pickle.dump(pdf_texts, f)\n",
        "\n",
        "    print(\"âœ… PDF texts cached to Drive\")\n",
        "\n",
        "# -------------------------\n",
        "# 5ï¸âƒ£ Load or upload CSV (ONCE)\n",
        "# -------------------------\n",
        "if os.path.exists(CSV_CACHE_PATH):\n",
        "    with open(CSV_CACHE_PATH, \"rb\") as f:\n",
        "        benchmark_df = pickle.load(f)\n",
        "    print(\"âœ… Loaded cached benchmark CSV from Drive\")\n",
        "else:\n",
        "    from google.colab import files\n",
        "    print(\"ðŸ“¤ Upload ONLY the benchmark CSV now\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    csv_file = [f for f in uploaded if f.lower().endswith(\".csv\")][0]\n",
        "    benchmark_df = pd.read_csv(csv_file)\n",
        "\n",
        "    benchmark_df = benchmark_df.drop(\n",
        "        columns=[c for c in ['Benchmark paper', 'Code repository', 'Dataset '] if c in benchmark_df.columns],\n",
        "        errors=\"ignore\"\n",
        "    )\n",
        "\n",
        "    with open(CSV_CACHE_PATH, \"wb\") as f:\n",
        "        pickle.dump(benchmark_df, f)\n",
        "\n",
        "    print(\"âœ… CSV cached to Drive\")\n",
        "\n",
        "# -------------------------\n",
        "# 6ï¸âƒ£ Query processing (NO LLM)\n",
        "# -------------------------\n",
        "def answer_query(query, pdf_texts, df):\n",
        "    query = query.lower()\n",
        "    keywords = re.findall(r\"\\w+\", query)\n",
        "\n",
        "    # --- Search PDFs ---\n",
        "    pdf_hits = []\n",
        "    for text in pdf_texts:\n",
        "        score = sum(text.lower().count(k) for k in keywords)\n",
        "        if score > 0:\n",
        "            pdf_hits.append((score, text[:500]))\n",
        "\n",
        "    pdf_hits.sort(reverse=True)\n",
        "\n",
        "    # --- Search CSV ---\n",
        "    csv_hits = df[df.apply(\n",
        "        lambda row: any(k in str(row).lower() for k in keywords),\n",
        "        axis=1\n",
        "    )]\n",
        "\n",
        "    # --- Build answer ---\n",
        "    answer = \"\\nðŸ“Œ RESULTS BASED ON DATA\\n\\n\"\n",
        "\n",
        "    if not csv_hits.empty:\n",
        "        answer += \"ðŸ”¹ Relevant Models from Benchmarks:\\n\"\n",
        "        answer += csv_hits.head(5).to_string(index=False)\n",
        "        answer += \"\\n\\n\"\n",
        "\n",
        "    if pdf_hits:\n",
        "        answer += \"ðŸ”¹ Evidence from Research Papers:\\n\"\n",
        "        answer += pdf_hits[0][1]\n",
        "        answer += \"\\n\"\n",
        "\n",
        "    if not pdf_hits and csv_hits.empty:\n",
        "        answer += \"No relevant information found.\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "# -------------------------\n",
        "# 7ï¸âƒ£ User query\n",
        "# -------------------------\n",
        "question = input(\"Enter your research question: \")\n",
        "result = answer_query(question, pdf_texts, benchmark_df)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(result)\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz3so8EHHmYI",
        "outputId": "96c8e8a2-d03b-4265-99c8-c914e429b2d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "âœ… Loaded cached PDF texts from Drive\n",
            "âœ… Loaded cached benchmark CSV from Drive\n",
            "Enter your research question: Which LLM performs best for question answering tasks?\n",
            "\n",
            "==================================================\n",
            "\n",
            "ðŸ“Œ RESULTS BASED ON DATA\n",
            "\n",
            "ðŸ”¹ Relevant Models from Benchmarks:\n",
            "           Name                              Type                                                                                                                                                                           Description Number of examples                License\n",
            "      MultiLoKo language & reasoning,multilingual                                                                                                         A new benchmark for evaluating multilinguality in LLMs covering 31 languages.              15500            MIT License\n",
            "FACTS Grounding                            safety                                                                         A measure of how accurately LLMs ground their responses in provided source material and avoid hallucinations.               1719              CC-BY-4.0\n",
            "     Graphwalks              language & reasoning                  A dataset for evaluating multi-hop long-context reasoning. In Graphwalks, the model is given a graph represented by its edge list and asked to perform an operation.               1150            MIT License\n",
            "         NoLiMa       information retrieval & RAG                      Extended NIAH, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack.               7540 Adobe Research License\n",
            " MultiChallenge           conversation & chatbots Evaluates LLMs on conducting multi-turn conversations with human users across 4 challenges: instruction retention, inference memory, reliable versioned editing, and self-coherence.                 273       see dataset page\n",
            "\n",
            "ðŸ”¹ Evidence from Research Papers:\n",
            "A Survey on Large Language Model Benchmarks\n",
            "Shiwen Ni1, Guhong Chen1, 2, Shuaimin Li1, Xuanang Chen9, Siyi Li1, 4, Bingli Wang6\n",
            "Qiyao Wang1, 3, Xingjian Wang5, Yifan Zhang7, Liyang Fan8\n",
            "Chengming Li10, Ruifeng Xu11, Le Sun9, Min Yang1, 12, *\n",
            "1Shenzhen Key Laboratory for High Performance Data Mining,\n",
            "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n",
            "2Southern University of Science and Technology3University of Chinese Academy of Sciences\n",
            "4University of Science and Technology \n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}